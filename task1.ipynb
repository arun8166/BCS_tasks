{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arun8166/BCS_tasks/blob/main/task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKg47GBdXY9"
      },
      "source": [
        "# **Brain and Cognitive Society, IIT Kanpur**\n",
        "## **Introduction to Deep Learning Workshop**\n",
        "**This python notebook is an assingment on ML/DL**\n",
        "\n",
        "In this assingment you will solve a **regression** problem of predicting House prices using basic python libraries, and build a **neural network** for handwritten digit identification using **TensorFlow**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Regression**\n",
        "We will use Linear regression for predicting house prices\n",
        "\n",
        "We are using a Kaggle dataset- https://www.kaggle.com/harlfoxem/housesalesprediction"
      ],
      "metadata": {
        "id": "FSRalact4xj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets import required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "J8MRCcJY2btt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Preparation**"
      ],
      "metadata": {
        "id": "1MzAhMmE6UHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Execute this cell for loading dataset in a pandas dataframe\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=16x6-8Znn2T50zFwVvKlzsdN7Jd1hpjct' -O Linear_regression_dataset\n",
        "\n",
        "data_df = pd.read_csv(\"Linear_regression_dataset\")"
      ],
      "metadata": {
        "id": "LiWI-2py554R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"(No of rows, No of Columns) = \",data_df.shape)\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "xsyWhm3MT3S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets have a quick Look at dataset\n",
        "\n",
        "print(\"(No of rows, No of Columns) = \",data_df.shape)\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "TAodxbYX7AKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So there are **19** features (of course we will not use id as feature :) ), and 1 variable to predict(price)\n",
        "\n",
        "But note that the **date** column contain strings so first we will remove T00.. part from it and than convert it to numpy array."
      ],
      "metadata": {
        "id": "gsJaooGZ7pUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['date'] = data_df['date'].str.replace('T000000', '')\n",
        "\n",
        "data_array=np.array(data_df)\n",
        "data_array=np.delete(data_array,0,1)\n",
        "data_array=data_array.astype(np.int)\n",
        "print(data_array)\n",
        "\n",
        "assert (data_array.shape == (21613,20))\n",
        "\n",
        "data_df.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vCPIoOV8UHs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the next task is **normalization**.\n",
        "\n",
        "We will scale each column of dataset by x -> (x-u)/s\n",
        "\n",
        "where u is mean(x), and s is standard deviation of u"
      ],
      "metadata": {
        "id": "xsBZxZ4x-oBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mean_array = np.mean(data_array,0)                                        # this should be an array, each entry should be mean of a column\n",
        "sd_array = np.mean(data_array,0)                                    # this should be an array, each entry should be standard deviation of a column\n",
        "\n",
        "data_array_norm = (data_array - mean_array)/sd_array\n",
        "print(data_array_norm)\n",
        "\n",
        "print(data_array_norm.shape)"
      ],
      "metadata": {
        "id": "u7GZV-0T_zCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step is to make train and test dataset and to create seperate vector for price"
      ],
      "metadata": {
        "id": "VCQTrNIgAlPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data_array_norm[:,1]                                                                                                            # extract the price column from data\n",
        "\n",
        "x_array_norm = np.delete(data_array_norm,1,1)                                                                                                      # delete the price column from data_array_norm. Hint: use np.delete()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_array_norm,labels,test_size=0.15,random_state=42,shuffle=True)    # splitting data into test and train set.\n",
        "\n",
        "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
      ],
      "metadata": {
        "id": "dJyX5QOFBRg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u0YPCk1Wt7l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loss and gradient descent**\n",
        "We will use mean squared error(MSE) as loss\n",
        "\n",
        "Use the gradient descent algorithm which you learned from tutorials\n",
        "\n",
        "Your task is to complete the following functions"
      ],
      "metadata": {
        "id": "iAdW-22ZDcdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y_pred,y_true):\n",
        "\n",
        "  mse = np.mean(np.square(np.subtract(y_true,y_pred)))     #mse = mean_squared_error(y_true,y_pred)\n",
        "\n",
        "\n",
        "  return mse"
      ],
      "metadata": {
        "id": "zsewTW0IvjJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def y(x,a,b):\n",
        "  m,n = x.shape\n",
        "  \n",
        "  y_pred = np.dot(x,a) + b\n",
        "  y_pred = y_pred.astype(np.float)\n",
        "\n",
        "  #assert(y_pred.shape == (m,))\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "PuNnYzHZrYYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(x,a,b,y_true):\n",
        "  \"\"\"\n",
        "  This function shoud return gradient of loss\n",
        "  input:\n",
        "  x: [array] the feature vector of shape (m,n)\n",
        "  a: [array] weights of shape (n,)\n",
        "  b: [scalar] bias\n",
        "  y_true: [array] ground truth of shape (m,)\n",
        "\n",
        "  output:\n",
        "  grad: [tuple] a tuple (derivative with respect to a[array of shape(n,)], derivative with respect to b[scalar])\n",
        "  \"\"\"\n",
        "  m,n = x.shape\n",
        "  yp = y(x,a,b)\n",
        "  y_true = y_true.astype(np.float)\n",
        "  d=[1]*n\n",
        "  for k in range(0,n,1):\n",
        "    d[k] = 2*np.mean(x[:,k]*np.subtract(yp,y_true))\n",
        "  da = np.array(d)                                   # write code to calculate derivative of loss with respect to a\n",
        "  db = 2*np.mean(np.subtract(yp,y_true))             # write code to calculate derivative of loss with respect to b\n",
        "\n",
        "  assert(da.shape ==(n,))\n",
        "  return (da,db)"
      ],
      "metadata": {
        "id": "lYnPROu8Gxwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x,y_true,learning_rate=0.01,epochs = 100):\n",
        "  \"\"\"\n",
        "  This function perfroms gradient descent and minimizes loss\n",
        "  input:\n",
        "  x: [array] the feature vector of shape (m,n)\n",
        "  y_true: [array] ground truth of shape (m,)\n",
        "  \n",
        "  output:\n",
        "  loss: [array] of size (epochs,)\n",
        "  weights: [tuple] (a,b)\n",
        "  \"\"\"\n",
        "  m,n = x.shape\n",
        "  loss_mse = []\n",
        "  v = [0]*n                                 # initialize empty list to store loss\n",
        "  a = np.array(v) \n",
        "  n, = a.shape                              # initialize a- weights and b- bias\n",
        "  b = 0 \n",
        "\n",
        "  for i in range(epochs):\n",
        "    # calculate derivative using gradient() function\n",
        "    # apply gradient descent now to update a and b\n",
        "    for j in range(0,n,1):\n",
        "      a[j] = a[j] - learning_rate*gradient(x,a,b,y_true)[0][j]\n",
        "    b = b - learning_rate*gradient(x,a,b,y_true)[1]\n",
        "\n",
        "    l_mse = loss(y(x,a,b),y_true)                                # calculate loss at this point\n",
        "    loss_mse.append(l_mse)\n",
        "\n",
        "    print(\"Epoch \",i+1,\" Completed!\",\"loss = \",l_mse)\n",
        "  \n",
        "  print(\"Training completed!!\")\n",
        "\n",
        "  assert(a.shape==(n,))\n",
        "\n",
        "  return (loss_mse,a,b)"
      ],
      "metadata": {
        "id": "km_z3ojKKQdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training** "
      ],
      "metadata": {
        "id": "VsR5XLl_WVu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 150             # tweak this!!!\n",
        "learn_rate = 0.01     # choose learning rate wisely otherwise loss may diverge!!\n",
        "\n",
        "train_loss,a,b = gradient_descent(x_train,y_train,learn_rate,epochs)"
      ],
      "metadata": {
        "id": "Yz389vsZ2h3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation and Visualization**\n",
        "Lets plot how loss varies with epochs\n"
      ],
      "metadata": {
        "id": "TDH-7NHQT50f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = gradient_descent(x_test,y_test,learn_rate,epochs)\n",
        "print(\"Loss on test data = \",test_loss)\n",
        "epo=np.linspace(0,epochs,epochs)\n",
        "\n",
        "plt.plot(epo,test_loss[0])                   # plot loss versus epochs\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "c_GynnWhjchN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zaRqYA6i5F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deep Learning**\n",
        "In this section We will build a simple multilayer perceptron network(**MLP**) in TensorFlow"
      ],
      "metadata": {
        "id": "iQ5wPjPb57pb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGoZEKIbdXZD"
      },
      "outputs": [],
      "source": [
        "# Lets import the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThuZ51gEdXZF"
      },
      "source": [
        "### **Load Dataset**\n",
        "We will be using MNIST dataset of handwritten digits\n",
        "\n",
        "Just run the cell below to load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj3J8Dp-dXZG"
      },
      "outputs": [],
      "source": [
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(\"No. of training examples = \",x_train.shape[0])\n",
        "print(\"Size of each image in dataset = \",x_train.shape[1:])\n",
        "print(\"No. of test examples = \",x_test.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX9xW4ardXZG"
      },
      "outputs": [],
      "source": [
        "# Run this cell to visualize some of the images from dataset\n",
        "\n",
        "n = 10    # = no. of images to visualize\n",
        "\n",
        "index = np.random.choice(x_train.shape[0],10)  # choose random index\n",
        "print(\"label: \",end=\"\")\n",
        "\n",
        "for i,ind in enumerate(index):\n",
        "    plt.subplot(1,n,i+1)\n",
        "    plt.imshow(x_train[ind],cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    print(y_train[ind],end=\"       \")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQcA9i9YdXZH"
      },
      "source": [
        "#### Preprocess dataset\n",
        "Since we are building a MLP model the input to the model should be a vector rather than a 28 by 28 matrix.\n",
        "\n",
        "So your **First Task** is to flatten the images\n",
        "\n",
        "(Hint: use *reshape()* method of arrays...)\n",
        "\n",
        "Next, create validation dataset out of training dataset.\n",
        "\n",
        "You can use 50K images for training and 10K for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXnkfE6gdXZI"
      },
      "outputs": [],
      "source": [
        "# Flatten the images into 1-d vectors\n",
        "x_train = x_train/256\n",
        "x_test = x_test/256\n",
        "\n",
        "x_train_flatten = x_train.reshape(60000,784)  \n",
        "x_test                                     # flatten the images of training set\n",
        "x_test_flatten = x_test.reshape(10000,784)                                        # flatten th eimages of test set\n",
        "\n",
        "\n",
        "# Divide the training data into training and validation data....\n",
        "\n",
        "n_validation = 10000 \n",
        "x_train, x_validation, y_train, y_validation = train_test_split(x_train,y_train,test_size=1/6,random_state=42,shuffle=True)\n",
        "x_validation = x_validation.reshape(10000,784)\n",
        "x_train_flatten = x_train.reshape(50000,784)                                      # choose number of images to be used for validation\n",
        "print(x_train.shape, y_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMGl2aq3dXZJ"
      },
      "source": [
        "### **Build a model**\n",
        "You can choose whatever architechure you want, but ensure that it is **not too deep** as that will take too much time to train and **not too shallow** as that will give very low accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7yr3nwTdXZK"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')]\n",
        "    )\n",
        "\n",
        "# Make a graphical representation of the model...\n",
        "keras.utils.plot_model(model,show_shapes=True)\n",
        "model.summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oDNvKB6dXZL"
      },
      "source": [
        "#### Compile and Train\n",
        "Choose an optimizer- method that minimizes loss function\n",
        "\n",
        "**adam** optimizer is one of the popular choices. You should read about these online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DM9i_F5dXZL"
      },
      "outputs": [],
      "source": [
        "from keras import losses, optimizers, metrics\n",
        "model.compile(optimizer = \"adam\", loss = 'sparse_categorical_crossentropy',metrics=[\"accuracy\"])\n",
        "\n",
        "n_epochs = 20              # set number of epochs\n",
        "batch_size = 512            # you can tweak with these parametrs\n",
        "history = model.fit(x_train_flatten,y_train,epochs=20,validation_data=(x_validation,y_validation))\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QTWTtoVdXZM"
      },
      "source": [
        "### **Evaluate**\n",
        "Evaluate your model on test data.\n",
        "\n",
        "And Show some results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhuBGWg-dXZM"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test_flatten,y_test, 20)\n",
        "print(\"Loss = \",results[0])\n",
        "print(\"Accuracy = \",results[1]*100,\"%\")\n",
        "\n",
        "# Plot Accuracy...\n",
        "plt.plot([i for i in range(n_epochs)],history.history['accuracy'], label=\"Training accuracy\")\n",
        "plt.plot([i for i in range(n_epochs)],history.history['val_accuracy'],label=\"validation accuracy\" )\n",
        "plt.title(\"Model accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Similarly write code to plot loss...\n",
        "plt.plot([i for i in range(n_epochs)],history.history['loss'], label=\"loss\")\n",
        "plt.title(\"Model loss\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hjr0CBhdXZN"
      },
      "source": [
        "Lets show our results on images from testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ta8UWQFoxNF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEs1cVAHdXZN"
      },
      "outputs": [],
      "source": [
        "n = 10   # = no. of images to see predictions on\n",
        "index = np.random.choice(x_test.shape[0],10)  # choose random index from test data\n",
        "print(\"label: \",end=\"\")\n",
        "\n",
        "for i,ind in enumerate(index):\n",
        "    plt.subplot(1,n,i+1)\n",
        "    plt.imshow(x_test[ind],cmap=\"gray\")             # fill code to show images from test set\n",
        "    plt.axis(\"off\")\n",
        "    print(y_test[ind],end=\"       \")\n",
        "print(index)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "plt.show()\n",
        "print(\"Predicted value: \")\n",
        "\n",
        "# Now lets print the predictions\n",
        "\n",
        "for i,ind in enumerate(index):\n",
        "    # write code to predict and print digit in image\n",
        "    # Hint: the output of the model is a 10-d vector which gives probabilties\n",
        "    # The digit in the image would be the class for which probability is hghest..\n",
        "    di = model.predict(x_test_flatten)\n",
        "    digit = np.argmax(di[ind])\n",
        "\n",
        "    #digits = model.predict(x_test_flatten)\n",
        "    print(digit,end=\"       \")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne8-aBvHdXZO"
      },
      "source": [
        "That's it you have completed the assignment !!\n",
        "\n",
        "We hope that you learned something from this exercise\n",
        "\n",
        "### Credits:\n",
        "\n",
        "**Leaders:**\n",
        "\n",
        "Mohit Kulkarni\n",
        "\n",
        "Shivanshu Tyagi\n",
        "\n",
        "**Scretaries:**\n",
        "\n",
        "Sahil Bansal\n",
        "\n",
        "Shashwat Gupta\n",
        "\n",
        "Rashmi Sharma"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}